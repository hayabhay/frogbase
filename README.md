# Sutitle generation using OpenAI's Whisper

This is a simple [Streamlit UI](https://streamlit.io/) for [OpenAI's Whisper speech-to-text model](https://openai.com/blog/whisper/).
It let's you download and transcribe media from YouTube videos, playlists, or local files with specific settings.
You can then browse, filter, and search through your saved audio files.
Feel free to raise an issue for bugs or feature requests or send a PR.

é€™æ˜¯ä¸€å€‹ç°¡å–®çš„ [Streamlit UI](https://streamlit.io/) ï¼Œç”¨æ–¼  [OpenAIçš„Whisper](https://openai.com/blog/whisper/) èªéŸ³è½‰æ–‡å­—æ¨¡å‹ã€‚
å®ƒå…è¨±æ‚¨å¾YouTubeè¦–é »ã€æ’­æ”¾åˆ—è¡¨æˆ–æœ¬åœ°æ–‡ä»¶ä¸‹è¼‰å’Œè½‰éŒ„åª’é«” (ä¸€å€‹æª”æ¡ˆé™åˆ¶ç‚º200MB)ã€‚
ç„¶å¾Œï¼Œæ‚¨å¯ä»¥ç€è¦½ã€éæ¿¾å’Œæœç´¢æ‚¨ä¿å­˜çš„éŸ³é »æ–‡ä»¶ã€‚éš¨æ™‚æ­¡è¿æå‡ºéŒ¯èª¤æˆ–åŠŸèƒ½è¦æ±‚ï¼Œæˆ–ç™¼é€ PRã€‚

https://user-images.githubusercontent.com/6735526/216852681-53b6c3db-3e74-4c86-806f-6f6774a9003a.mp4

## Setup
This was built & tested on Python 3.11 but should also work on Python 3.8+ as with the original [Whisper repo](https://github.com/openai/whisper)).
You'll need to install `ffmpeg` on your system. Then, install the requirements with `pip`.

```
# Install pytorch if you don't have it
# sudo conda install pytorch
pip install -r requirements.txt
pip install git+https://github.com/openai/whisper.git
```

## Usage

1. Once you're set up, you can run the app with:

```
streamlit run app/01_ğŸ _Home.py
```

This will open a new tab in your browser with the app. You can then select a YouTube URL or local file & click "Run Whisper" to run the model on the selected media.

If the tab doesn't open, please use the URL: ```https://localhost:8501``` in your browser.

2. If you are not satisfied with the output, click on 'âš™ï¸Settings' on the left, then you can fine-tune the inference of Whisper model.

Important âš™ï¸Settings F.Y.I :
- Model: the model branch you want to use, defult: ```medium```
- language: the language of transcription, default: ```zh``` (ä¸­æ–‡)
- No Speech Threshold: how strictly we are in excluding non-speech detection, default ```0.4``` (lower level are more strict)
- Condition on previous text: whether the model will be affected by last text, default: ```True```


## Hosting

If you want to host it on a server with dynamic IP, you can install ```ngrok``` for forwarding your IP out. 
So you can access it anywhere via a random url like: ```https://b9f1-458-19-17-41.jp.ngrok.io``` 


ğŸ”¥You can try our demo [here](https://whispersubtitle.aiacademy.tw)

Special thanks to [<img src=https://i.imgur.com/bTHUPca.png width=300>](https://en.aiacademy.tw/) for the server.

## Changelog
See [Commits](https://github.com/ShuYuHuang/whisper-subtitle/commits) for detailed changes.

Version summary will be provided in [Release](https://github.com/ShuYuHuang/whisper-subtitle/releases).

The changelog of the original vertion can be found [in this file](https://github.com/hayabhay/whisper-ui/blob/main/CHANGELOG.md).

## License
- Whisper: [MIT](https://github.com/openai/whisper/blob/main/LICENSE)
- Streamlit: [Apache 2.0](https://github.com/streamlit/streamlit/blob/develop/LICENSE).
- else: [MIT](https://github.com/hayabhay/whisper-ui/blob/main/LICENSE).

## Reference
I forked the original version of the interfaces form https://github.com/hayabhay/whisper-ui. 

They actually did a great job for forming a manage systme of subtitles: search engine, transcript viewer, settings

The original version aims to demonstrate the power of Whisper, especially for short films in youtube for local use.

My goal is to provide a service for a bunch of clinets to make subtitles for long videos like meeting records, courses and movies.